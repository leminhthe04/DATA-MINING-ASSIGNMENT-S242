{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[     nan      nan      nan]\n",
      "  [7.00e+00 4.30e-03 6.50e+01]\n",
      "  [5.90e+01 1.48e-02 6.93e+01]\n",
      "  ...\n",
      "  [     nan      nan      nan]\n",
      "  [1.10e+01 6.30e-03 6.73e+01]\n",
      "  [2.50e+01 1.15e-02      nan]]\n",
      "\n",
      " [[     nan      nan      nan]\n",
      "  [1.00e+01 7.60e-03 6.49e+01]\n",
      "  [5.40e+01 1.23e-02 6.94e+01]\n",
      "  ...\n",
      "  [     nan      nan      nan]\n",
      "  [5.00e+00 2.40e-03 6.63e+01]\n",
      "  [1.60e+01 6.90e-03      nan]]\n",
      "\n",
      " [[     nan      nan      nan]\n",
      "  [1.10e+01 7.60e-03 6.50e+01]\n",
      "  [5.10e+01 1.34e-02 6.83e+01]\n",
      "  ...\n",
      "  [     nan      nan      nan]\n",
      "  [1.40e+01 7.00e-03 6.63e+01]\n",
      "  [2.40e+01 9.90e-03      nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.10e+01      nan      nan]\n",
      "  [7.00e+00 3.40e-03 6.50e+01]\n",
      "  [1.28e+02 3.18e-02 7.18e+01]\n",
      "  ...\n",
      "  [     nan      nan      nan]\n",
      "  [2.30e+01 1.26e-02 6.71e+01]\n",
      "  [2.40e+01 1.12e-02      nan]]\n",
      "\n",
      " [[7.00e+00      nan      nan]\n",
      "  [7.00e+00 3.30e-03 6.50e+01]\n",
      "  [1.48e+02 4.55e-02 6.88e+01]\n",
      "  ...\n",
      "  [     nan      nan      nan]\n",
      "  [2.30e+01 1.18e-02 6.73e+01]\n",
      "  [2.00e+01 9.80e-03      nan]]\n",
      "\n",
      " [[8.00e+00      nan      nan]\n",
      "  [6.00e+00 2.90e-03 6.50e+01]\n",
      "  [1.47e+02 4.20e-02 6.84e+01]\n",
      "  ...\n",
      "  [     nan      nan      nan]\n",
      "  [2.60e+01 1.36e-02 6.70e+01]\n",
      "  [2.40e+01 9.00e-03      nan]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('p01_done.npy', allow_pickle=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           node_id  time_slot  traffic_volume  occupancy_rate  speed\n",
      "0                0          0             NaN             NaN    NaN\n",
      "1                1          0             7.0          0.0043   65.0\n",
      "2                2          0            59.0          0.0148   69.3\n",
      "3                3          0           237.0          0.1463   69.2\n",
      "4                4          0            43.0          0.0205   65.0\n",
      "...            ...        ...             ...             ...    ...\n",
      "151526011    16967       8927            46.0          0.0118   68.6\n",
      "151526012    16968       8927             4.0             NaN    NaN\n",
      "151526013    16969       8927             NaN             NaN    NaN\n",
      "151526014    16970       8927            26.0          0.0136   67.0\n",
      "151526015    16971       8927            24.0          0.0090    NaN\n",
      "\n",
      "[151526016 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_time_slots, num_nodes, _ = data.shape\n",
    "\n",
    "time_idx, node_idx = np.meshgrid(\n",
    "    np.arange(num_time_slots), np.arange(num_nodes), indexing=\"ij\"\n",
    ")\n",
    "\n",
    "# Flatten data\n",
    "df = pd.DataFrame({\n",
    "    \"node_id\": node_idx.ravel(),\n",
    "    \"time_slot\": time_idx.ravel(),\n",
    "    \"traffic_volume\": data[:, :, 0].ravel(),\n",
    "    \"occupancy_rate\": data[:, :, 1].ravel(),\n",
    "    \"speed\": data[:, :, 2].ravel()\n",
    "})\n",
    "\n",
    "print(df) # show all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                NaN Count  NaN Percentage (%)\n",
      "node_id                 0            0.000000\n",
      "time_slot               0            0.000000\n",
      "traffic_volume   26459359           17.461925\n",
      "occupancy_rate   33404359           22.045296\n",
      "speed            56761959           37.460207\n"
     ]
    }
   ],
   "source": [
    "# count rows have NaN of each column\n",
    "nan_count = df.isna().sum()\n",
    "nan_percentage = (nan_count / len(df)) * 100\n",
    "\n",
    "nan_info = pd.DataFrame({\n",
    "    'NaN Count': nan_count,\n",
    "    'NaN Percentage (%)': nan_percentage\n",
    "})\n",
    "\n",
    "print(nan_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           node_id  time_slot  traffic_volume  occupancy_rate  speed\n",
      "1                1          0             7.0          0.0043   65.0\n",
      "2                2          0            59.0          0.0148   69.3\n",
      "3                3          0           237.0          0.1463   69.2\n",
      "4                4          0            43.0          0.0205   65.0\n",
      "5                5          0            27.0          0.0141   65.0\n",
      "...            ...        ...             ...             ...    ...\n",
      "151526007    16963       8927            17.0          0.0081   65.2\n",
      "151526009    16965       8927           113.0          0.0193   68.4\n",
      "151526010    16966       8927            92.0          0.0321   66.9\n",
      "151526011    16967       8927            46.0          0.0118   68.6\n",
      "151526014    16970       8927            26.0          0.0136   67.0\n",
      "\n",
      "[94764057 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop NaN\n",
    "df = df.dropna()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract features\n",
    "features = ['traffic_volume', 'occupancy_rate', 'speed']\n",
    "X = df[features]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Use elbow method to find the optimal number of clusters\n",
    "inertia = []\n",
    "k_range = range(1, 10)\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(X_scaled)\n",
    "    inertia.append(km.inertia_)\n",
    "    print(f\"k = {k}, inertia = {km.inertia_}\")\n",
    "\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Inertia (Distortion)\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# After finding the optimal number of clusters (k=3) use it to cluster the data\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labelled data to a parquet file\n",
    "df.to_parquet(\"traffic_data_labelled.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          node_id  time_slot  traffic_volume  occupancy_rate  speed  cluster\n",
      "0               1          0             7.0          0.0043   65.0        2\n",
      "1               2          0            59.0          0.0148   69.3        2\n",
      "2               3          0           237.0          0.1463   69.2        0\n",
      "3               4          0            43.0          0.0205   65.0        2\n",
      "4               5          0            27.0          0.0141   65.0        2\n",
      "...           ...        ...             ...             ...    ...      ...\n",
      "94764052    16963       8927            17.0          0.0081   65.2        2\n",
      "94764053    16965       8927           113.0          0.0193   68.4        2\n",
      "94764054    16966       8927            92.0          0.0321   66.9        2\n",
      "94764055    16967       8927            46.0          0.0118   68.6        2\n",
      "94764056    16970       8927            26.0          0.0136   67.0        2\n",
      "\n",
      "[94764057 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"traffic_data_labelled.parquet\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "2    64390657\n",
      "0    26897335\n",
      "1     3476065\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# count number of samples in each cluster\n",
    "print(df['cluster'].value_counts())\n",
    "\n",
    "# Choose randomly 100000 samples to plot, because plot all the dataset is too slow\n",
    "random_samples = df.sample(n=100000)\n",
    "\n",
    "sns.pairplot(random_samples[features + ['cluster']], hue='cluster', palette='Set2')\n",
    "plt.suptitle(\"KMeans Clustering Result\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df[['traffic_volume', 'occupancy_rate', 'speed']]\n",
    "y = df['cluster']\n",
    "\n",
    "# Split dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the split data to a numpy file\n",
    "\n",
    "np.save(\"X_train.npy\", X_train_scaled)\n",
    "np.save(\"X_test.npy\", X_test_scaled)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "np.save(\"X_train_scaled.npy\", X_train_scaled)\n",
    "np.save(\"X_test_scaled.npy\", X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "X_train_scaled = np.load(\"X_train_scaled.npy\")\n",
    "X_test_scaled = np.load(\"X_test_scaled.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 5360644     7402     9432]\n",
      " [    3029   690002     3453]\n",
      " [   12470      235 12866145]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   5377478\n",
      "           1       0.99      0.99      0.99    696484\n",
      "           2       1.00      1.00      1.00  12878850\n",
      "\n",
      "    accuracy                           1.00  18952812\n",
      "   macro avg       1.00      1.00      1.00  18952812\n",
      "weighted avg       1.00      1.00      1.00  18952812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create a logistic regression model for multi-class classification\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 75811245, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score -1.259271\n",
      "[LightGBM] [Info] Start training from score -3.305946\n",
      "[LightGBM] [Info] Start training from score -0.386436\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's multi_logloss: 0.151036\n",
      "[20]\tvalid_0's multi_logloss: 0.0503812\n",
      "[30]\tvalid_0's multi_logloss: 0.0226845\n",
      "[40]\tvalid_0's multi_logloss: 0.0137958\n",
      "[50]\tvalid_0's multi_logloss: 0.010383\n",
      "[60]\tvalid_0's multi_logloss: 0.00875281\n",
      "[70]\tvalid_0's multi_logloss: 0.00778902\n",
      "[80]\tvalid_0's multi_logloss: 0.00713582\n",
      "[90]\tvalid_0's multi_logloss: 0.00666997\n",
      "[100]\tvalid_0's multi_logloss: 0.00630132\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's multi_logloss: 0.00630132\n",
      "Confusion Matrix:\n",
      " [[ 5366231     2865     8382]\n",
      " [    3894   692297      293]\n",
      " [    9253      243 12869354]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   5377478\n",
      "           1       1.00      0.99      0.99    696484\n",
      "           2       1.00      1.00      1.00  12878850\n",
      "\n",
      "    accuracy                           1.00  18952812\n",
      "   macro avg       1.00      1.00      1.00  18952812\n",
      "weighted avg       1.00      1.00      1.00  18952812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# Create a LightGBM model for multi-class classification\n",
    "model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=3,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with early stopping and logging, use original data, not scaled data\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='multi_logloss',\n",
    "    callbacks=[early_stopping(stopping_rounds=10), log_evaluation(10)]\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 5213289    18533   145656]\n",
      " [   42273   651458     2753]\n",
      " [   36369        0 12842481]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98   5377478\n",
      "           1       0.97      0.94      0.95    696484\n",
      "           2       0.99      1.00      0.99  12878850\n",
      "\n",
      "    accuracy                           0.99  18952812\n",
      "   macro avg       0.98      0.97      0.97  18952812\n",
      "weighted avg       0.99      0.99      0.99  18952812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create SGDClassifier model for multi-class classification\n",
    "sgd_svm_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Train the model with scaled data\n",
    "sgd_svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_sgd = sgd_svm_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_sgd))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74035/74035 - 192s - 3ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 0.9997 - val_loss: 9.1170e-04\n",
      "Epoch 2/10\n",
      "74035/74035 - 186s - 3ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9996 - val_loss: 0.0010\n",
      "Epoch 3/10\n",
      "74035/74035 - 182s - 2ms/step - accuracy: 0.9997 - loss: 0.0014 - val_accuracy: 0.9996 - val_loss: 8.3631e-04\n",
      "Epoch 4/10\n",
      "74035/74035 - 185s - 3ms/step - accuracy: 0.9998 - loss: 0.0013 - val_accuracy: 0.9998 - val_loss: 7.6260e-04\n",
      "Epoch 5/10\n",
      "74035/74035 - 187s - 3ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9998 - val_loss: 5.7382e-04\n",
      "Epoch 6/10\n",
      "74035/74035 - 181s - 2ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9998 - val_loss: 6.3858e-04\n",
      "Epoch 7/10\n",
      "74035/74035 - 185s - 3ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9998 - val_loss: 6.6533e-04\n",
      "Epoch 8/10\n",
      "74035/74035 - 186s - 3ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9998 - val_loss: 6.0947e-04\n",
      "Epoch 9/10\n",
      "74035/74035 - 188s - 3ms/step - accuracy: 0.9998 - loss: 0.0010 - val_accuracy: 0.9998 - val_loss: 5.4098e-04\n",
      "Epoch 10/10\n",
      "74035/74035 - 183s - 2ms/step - accuracy: 0.9998 - loss: 0.0010 - val_accuracy: 0.9996 - val_loss: 0.0014\n",
      "\u001b[1m592276/592276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 708us/step\n",
      "Confusion Matrix:\n",
      " [[ 5377323      154        1]\n",
      " [    1185   695008      291]\n",
      " [    6417        0 12872433]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   5377478\n",
      "           1       1.00      1.00      1.00    696484\n",
      "           2       1.00      1.00      1.00  12878850\n",
      "\n",
      "    accuracy                           1.00  18952812\n",
      "   macro avg       1.00      1.00      1.00  18952812\n",
      "weighted avg       1.00      1.00      1.00  18952812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Convert y_train, y_test to one-hot encode\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_test_cat = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Initial ANN model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_test_scaled, y_test_cat),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = tf.argmax(y_pred_prob, axis=1).numpy()\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
